<html>
  <head>
    <meta charset="utf-8" />

    <script
      type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"
    ></script>
  </head>
  <style>
    body {
      padding: 50px;
      background-color: #fffffc;
    }
    iframe {
      border: none;
      overflow: hidden;
    }
    h1 {
      font-size: 24px;
      color: #05bfdb;
    }
    p {
      font-size: 16px;
      line-height: 25px;
      color: #0b2027;
      text-indent: 2em;
    }
    li {
      font-size: 16px;
      line-height: 25px;
      color: #0b2027;
    }
    li::marker {
      color: #05bfdb;
    }
    .storyline {
      color: #088395;
    }
  </style>

  <body>
    <h1 style="font-size: 30px">Language Model领域论文可视化</h1>
    <h1>数据描述</h1>
    <p>
      OpenAlex是一个提供开放、全面且可机读的学术知识图谱的项目。它收录学术论文、作者、引用等方面的丰富数据并且结构化的存储它们。<br />
      OpenAlex提供API和应用程式两个方式提供，允许开发人员以编程方式查询和检索数据。且不用登录，完全免费。<br />
      openalex描述了7类主要学术实体以及一类辅助实体以及这些实际之间的联系。每类实体分别对应其目标：<br />
    </p>
    <li>
      works成果：包括论文、书籍、数据集等，会引用其他成果。目前有超过2亿4千万条数据，且每日以5万的速度增加
    </li>
    <li>authors作者：做出成果的人，目前有超过2亿条记录</li>
    <li>sources出处：刊载成果的期刊或资源库</li>
    <li>Institutions机构：（通过作者）与成果相关的大学或其他组织</li>
    <li>
      Concepts概念：对成果进行标签，并且将不同的概念进行层次化与学科联系，每个concepts有一个ID，如”语言模型“这个概念的ID是C137293760，他的子概念有n-gram、Time
      delay neural network等等。
    </li>
    <li>publisher出版商</li>
    <li>funders出资方</li>
    <li>Geo地理位置：非主要实体，用于以地理位置区分主要实体</li>
    <p>
      我们选定了“Language
      Model”这个概念对其进行查询，获取了这个概念之下27551条works（成果）每一条数据记录了ID、DOI、出版日期、标题、语言、作者群、作者机构、涉及到的概念、是否是开放数据（open-access）、可以从哪获取等等。<br />
      另外，我们也探索了Language
      Model相关的26种概念以及其从2012年迄今的演变。<br />
    </p>
    <p class="storyline">
      我们的可视化面向想要了解学习LM领域或者有意进入该领域开展研究的人群。
    </p>
    <p class="storyline">
      首先我们想通过累积发文量来呈现LM领域的总体发展趋势历程。
    </p>
    <h1>language model领域论文产出量</h1>
    <p>
      图以成果累计量为y轴，时间为x轴表现了语言模型领域历年来的积累情况。1960年代人工智能概念提出后，學界相继取得了一批令人瞩目的研究成果，如机器定理证明、跳棋程序等，掀起人工智能发展的第一个高潮。对应语言模型的相关研究，这时候也开始有一些学术成果发表。<br />
      并且在这五十年间整体呈现明显上升趋势。其中，明显的峰值出现在2000年、2005、2009；2000年之后的上升推估是由于网络技术的发展，推动了人工智能的创新研究与发展，不仅数据量大幅提升使得模型训练有更多提升，人们也有更多的需求去处理、分析网络上的文本。<br />
      2010年后随着计算机运算速度提升，深度学习开始兴起，近年语言模型中使用的类神经网络、word2vec等技术也是从这个时候开始出现，2018之后更有像BERT之类的NLP模型，因此整体的数据陡峭程度又见上升，<br />
      另外，本图可以放大检视时间轴，我们可以发现各年的发表集中在四月之前，这样的现象可能有以下原因：<br />
    </p>
    <li>1. openalex本身收录数据的偏差</li>
    <li>2. 语言模型会议集中在特定月份</li>
    <iframe
      width="100%"
      height="100%"
      src="workCnt/index.html"
      scrolling="no"
    ></iframe>
    <p class="storyline">
      那么近年LM领域出现热度攀升现象，是否来源于突破性论文的产生呢？
    </p>
    <h1>论文引用关系图</h1>
    <p>
      该图展示了1987-2023年被引量高于100且与Language
      Model概念相关性大于0.4的论文之间的引用关系。其中：点代表论文，点的大小代表论文的总被引量，点的颜色代表该论文与Language
      Model概念的相关性，相关性越大，颜色越深。点之间的连线代表论文的引用关系。<br />
      可以看到2015年后被引量较高的论文数量快速增长，几篇被引量极高的论文也在此时出现，如用于机器翻译的RNN
      Encoder-Decoder，以及BERT，RoBERTa等模型。
    </p>
    <iframe
      width="100%"
      height="600px"
      src="temporal-force-directed-graph-论文引用/index.html"
      scrolling="no"
    >
    </iframe>
    <p class="storyline">
      在论文这一微观层面上分析引用关系后，我们已经找到了领域内的重要论文。知识在引用网络上的传递，其中引用是面向过去，被引是面向未来，我们上升到宏观视角来说明发展连贯性与热度攀升也有关。
    </p>
    <h1>引文重叠比例热力图</h1>
    <p>
      ​以行来阅读热力图，对于每一个竖向轴上注明的年份（记作y），一行中所有方块在横向轴上对应的年份（记作x）均大于y。数据选取LM领域中70年代末（1978年）至今45年的各年份发表论文的引文（去重，设为c）。对于各方块，`(y,x)=[c_y∩c_x]/c_x`。指标引文重合度反映出y年研究在后续年份的辐射力，然而当我们以列来阅读热力图时，指标又可以反映出x年研究对先前年份的影射力。<br />
      图上可以明显看出对角线（x与y间隔仅1年）颜色较深，多数行从左至右都有逐步变浅的趋势，也就是随着间隔年份的增加，引文重合度降低。特别地，在1988年之前，对角线（连同右侧区域）颜色较浅，说明这段时间研究对未来研究辐射力弱、对过去研究也没有强映射力，总结为连贯性差。纵向来看，在1988年之后，多数列从上至下有逐步变深的趋势，对过去研究的映射范围在3年左右，比如最右列2023年研究与2020、2021、2022年研究引文都有较高重合度，推测其研究问题、思路方法也有一定连贯性。
    </p>
    <iframe
      width="100%"
      height="1000px"
      src="heatmap/index.html"
      scrolling="no"
    >
    </iframe>
    <p class="storyline">
      透过热度攀升现象看本质，接下来让我们从热度聚焦热点，看看模型领域的学者具体都在研究什么吧。
    </p>
    <h1>论文摘要词云图</h1>
    <p>
      收集了language
      model领域的摘要进行分词、去除停用词并且整理大小写之后绘的制词云图这个图展现了摘要内频率较高的词汇，越常出现的字字体越大，把鼠标移动到上面便会显示这个词在统计范围内出现了几次<br />
      最高频的字有language、model、models、performances等；从词汇中可以明显发现领域中很多机器学习术语如神经网络、预训练模型、数据集等等，并且从train、performance、evaluation、compare等词知道学界主要的几个研究面向有训练、优化、衡量结果与比较；另外这几年语言模型关注的几个问题有分类（classification）、多语言（multilingual），也能看到nlp常见的手法如encoding、retrieval、unsupervised<br />
      也能在词云中发现领域中比较常见的模型，如lstm、bert、transformer，可以理解成他们比较热门或者效果较好<br />
    </p>
    <iframe
      width="100%"
      height="500px"
      src="word-cloud/index.html"
      scrolling="no"
    >
    </iframe>
    <p class="storyline">
      论文摘要抽词构建词云，得到的结果是研究有关的方方面面杂糅的。我们采用openAlex定义的与论文相关的concepts将研究内容按主题概念清晰打标概括，再次进行展示。
    </p>
    <h1>不同concept随着年份和language model之间相关性的变化</h1>
    <p>
      这个图探索了Language
      Model相关的26种概念以及其从2012年迄今的演变，图中每个点是一个concept，点的颜色代表了在openalex的层次结构上的这个概念的分类等级，level1代表概念较大，level
      3是较为细分的概念，如Artificial intelligence是等级1，Deep
      learning是等级2，Word2vec是等级3，而我们的目标概念language
      model是等级2。<br />
      点的大小代表的是openalex给目标概念和这个概念之间的0-100的关联分数，分数越高代表越相关，也就是说，点的面积越大表示这个概念跟language
      model更为相关。<br />
      图的x轴当年该观念相关的作品被引用量，y轴则是当年该领域的论文量，反应了这些概念的学术重要程度和热门程度。<br />
      点击上方的年份按钮可以看到不同年份的数据结果，也可以比较多年来概念演变的趋势。能看到某些概念一直都很热门，在图中一直留在左下角，而总体被引量并没有那么高。但也有一些概念在这十年间有比较剧烈的变化：<br />
      比如tf-idf这个领域在2018年之前都保持着论文量少但是被引用量高的状况，可以推测这些领域的论文量较少，可能方法已经较为定型，大家研究相关概念的时候都是直接引用那些既有的论文而不是写出新的作品。但是在2019年开始有越来越多的新研究牵涉到这个概念。<br />
      另外Recurrent neural
      network在2012-2014年不仅热门，且相对被引量较高，可以说是学术界重点关注的概念。然而从2017年开始，这两项指标都年年衰减，到了2022，他的热度明显下降了很多。
    </p>
    <iframe
      width="100%"
      height="110%"
      src="related-concepts-change-through-time/index.html"
      scrolling="no"
    >
    </iframe>
    <p class="storyline">
      openAlex已经预定义了concepts之间的层级关系，但是这并不足以说明一篇研究通常选择聚焦的concept组合，我们可以通过密切的共现关系反映惯常的组合。
    </p>
    <h1>concept共现关系图</h1>
    <p>
      该图展示了Language Model概念的论文中，Language
      Model相关的26种概念之间的共现关系，其中点代表concept，点的大小代表含有该概念的论文数，点的颜色代表该概念与Language
      Model概念的相关性，相关性越大，颜色越深。点之间连线的粗细同时有两概念的论文数。<br />
      可以看到与Language Model最为相关的概念为n-gram，其次是speech
      recognition，AI和NLP之间的共现十分频繁，且这两个概念也是Language
      Model概念的论文出现最频繁的概念。
    </p>
    <iframe
      width="100%"
      height="100%"
      src="ForceDirectedGraph-concept共现/index.html"
      scrolling="no"
    >
    </iframe>
    <p class="storyline">
      在探讨了领域热度和其成因、领域内研究的内容和主题后，我们来看看领域学术资源在各个地区的分布，良好的区域学术环境将助推领域发展。
    </p>
    <h1>各大洲国家截止不同年份的累计发文量树图</h1>
    <p>
      可视化时间起始点选取1956年，也即"LM"上位concept"人工智能"元年。不同颜色代表不同大洲，背景方框标注了截止到各年世界LM领域的累积发文量，不同矩形面积与各国家发文量成正比例。<br />
      80年代之前，美国、英国累计发文量在全世界范围内遥遥领先；80年代至新世纪来临之前美国发文量继续猛增至接近50%的发文量为美国机构贡献，二亚洲的日本、韩国、中国大陆和中国台湾，欧洲的德国、法国、西班牙，还有意大利、荷兰等国发文量也大幅上升；2000年之后，LM领域蓬勃发展，经过23年积淀累计论文量如今已经达到2000年约25倍，呈现增速上涨趋势。<br />
      现在，中国机构发文量增长势头迅猛，美国的领先优势仍然明显，比如NLP领域热度极高的Attention和Bert都出自Google。但除南极洲之外的六大洲都有越来越多的国家机构学者加入到LM的研究队伍中。
    </p>
    <iframe
      width="100%"
      height="600px"
      src="animated-treemap/index.html"
      scrolling="no"
    >
    </iframe>
    <p class="storyline">我们将二维发文量引入到三维空间中展示。</p>
    <p>各国发文总量分布</p>
    <p>
      这幅3d地球仪各个国家对应的颜色为各国在Language
      Model领域的总发文数，色阶如左侧所示。
      由图可见，总发文量最高的国家为美国和中国。
    </p>
    <iframe
      width="100%"
      height="100%"
      src="geograph/3d地图.html"
      scrolling="no"
    ></iframe>
    <p class="storyline">
      学术环境的重要组成个体是学术机构，下面是机构的区域分布。
    </p>
    <h1>机构区域分布图</h1>
    <p>
      这幅地图上的点坐标对应的是各文章所属机构的地理位置。对于3d地球仪中提到发文总量最高的美国和中国，均可见东部发文量远超西部的现象。<br />
      除美国和中国外，本图中欧洲国家和日本的发文密度也极高，甚至高出美国和中国。但由于其国土面积较小，发文总量并不突出。
    </p>
    <iframe
      width="100%"
      height="800px"
      src="geograph/index.html"
      scrolling="no"
      style="margin-top: -100px; margin-bottom: -100px"
    ></iframe>
    <p class="storyline">
      在全球化时代，加强机构之间的交流和合作可以促进学术知识的交流和传播，推进领域再全球社会的发展。下面的图选取从1970年至2023年的发文数据，以十年为间隔构建时间轴。
    </p>
    <p>文章所属机构地点分布：</p>
    <p>
      图中点坐标对应的是各文章所属机构的地理位置。把鼠标放在坐标点上可看到文章名称。<br />
      可以看到前面的图点很稀疏，后面的图点很稠密。若点固定为同一个大小、同一个透明度，无法同时清晰地展示六张图。像现在，10s和20s的点就不大易于分辨。不过还好点的大小和透明度都是可以调节的。<br />
      如果不想看点图，想看区域颜色图呢？让它生成一下就好了。图的色阶是统一的，色阶最大值为时间轴全部节点中国家发文量的最大值。由图可见，从70年代至10年代各国发文量呈上升趋势。其中中国和美国发文量一直相对较高。<br />
      但是以发文量最大值设置色阶导致一个问题，发文量不高的地区颜色无法辨别。没关系，色阶的最大值最小值是可以修改的。是不是好多了？<br />
      如果想单独看每年各地区的发文量而不在意时间趋势，可以去除统一的色阶为每幅图单独设置色阶。这样每十年哪个地区发文最多就一目了然了。
    </p>
    <p>国际合作关系分布：</p>
    <p>
      由于按机构进行连线生成的图可读性较差，故按国家统计国际合作，同一篇论文中存在合作的国家之间以线相连。<br />
      合作次数越高线的不透明度越高。但由于前三张图合作次数普遍偏少，后两张图合作次数普遍偏多，难以找到可以同时适配每一张图的透明度，因此这里可以自己根据需要设置透明度。如果只想看那些国家间有合作可将透明度0.1，（点一下生成，再依此点一下时间轴的最后两个）当然线的粗细和线的弯曲度也是可以调节的。<br />
      从70年代至10年代国际合作的次数变多、范围变广，国际合作越来越密切。其中美国与欧洲以及美国与中国间合作最为密切。
    </p>
    <p>国际、国内合作总数分布：</p>
    <p>
      各图根据该国国际或国内合作次数进行染色。为每幅图单独设置色阶。<br />
      由图可见，两图除数量外，颜色分布相当一致。国家国际合作数与国内合作数高度相关。这可能是因为一国所发文章总量越高，其国际和国内合作数都会越高。<br />
      如果想在同一个色阶上对比国际和国内合作数，可修改色阶范围.（把左图和右图的色阶最大值都改成3000，分别点一下第一个生成）<br />
      由图可见从70年代至10年代国际与国内合作的次数均变多。同时研究人员相比于国际合作更倾向于国内合作。
    </p>
    <iframe width="100%" height="1500px" src="geograph/2d地图.html"></iframe>
  </body>
</html>
